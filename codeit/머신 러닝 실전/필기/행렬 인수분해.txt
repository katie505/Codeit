<행렬 인수분해>
- 곱했을 때 최대한 평점 행렬과 비슷하게 나오는 두 행렬을 구할 것
EX) 평점 행렬(R) = 유저 취향 행렬(대문자 세타) * 영화 속성 행렬(X)
=> 유저의 영화에 대한 평점 = 유저의 취향과 영화의 속성의 곱
- 평점 행렬을 인수분해하면 비어있는 평점들을 예측할 수 있음
=> 이렇게 하면 성능이 뛰어난 추천 시스템을 만들 수 있음
- 행렬에 빈값들이 있으면 행렬 인수분해를 하기 어려운 단점이 있음

- 경사 하강법을 통해 학습을 할 때 프로그램은 각 속성들이 어떤 것을 의미하는지 모름
- 각 데이터에 의미를 부여하지 않고 작은 임의의 값들로 초기화
- 어떤 속성을 쓸지는 정하지 않지만, 몇 개를 쓸지는 정해야 됨
=> 교차 검증과 그리드 서치를 사용해서 성능이 가장 좋은 개수로 정함

- 2006년 넷플릭스 영화/드라마 추천대회 & Simon Funk에 의해서 유명해짐
- 기존의 여러 다른 방식들보다 더 성능이 좋음
- 행렬 인수분해도 협업 필터링의 한 종류


행렬 인수분해 속성
- 내용 기반 추천에서는 선형 회귀 경사 하강법을 통해 속성을 입력 변수, 평점을 목표 변수로 해서 유저들의 취향 학습
- 행렬 인수분해에서는 유저 취향과 속성 둘 다 학습

데이터 표현하기
1. 영화 속성
- 변수 x를 사용해서 나타냄 = j번째 영화의 k번째 속성

2. 유저 취향
- 세타로 표현 = i번째 영화의 k번째 속성

3. 평점 예측값
- 유저i의 영화j에 대한 예측값 = 세타i(유저i의 취향)의 역행렬 * j번째 영화의 속성

4. 실제 평점 데이터
- y로 표현 = i번째 유저가 j번째 영화에 준 평점

5. 평점 데이터가 있는지 없는지
- r로 표현 = i번째 유저가 j번째 영화에 준 평점 유무 

손실 함수
- 행렬들이 얼마나 잘 예측을 하는지 평가하는 기준
- 제곱 오차 합이 작을수록 지금 취향과 속성 행렬 데이터가 좋다. 클수록 안 좋다
- 1/2*(평점 예측값 - 실제 평점 데이터의 제곱합)
=> 1/2 : 경사하강법 계산을 편하게 하기 위해서
=> 실제로 평점을 준 영화들에 대해서만 계산. 실제 평점이 없는 예측값들은 오차를 구할 수 없기 때문에 건너뜀

경사 하강법
- 유저 취향 세타 값들과 영화 속성 x값들을 잘 선택해서 둘을 곱했을 때 평점 데이터에 엄청 가깝게 나오게 하는 것
- 손실 함수의 아웃풋을 최대한 작게 만드는 세타와 x 찾기
- 손실함수를 각 취향과 각 속성에 대해서 편미분을 하여 세타와 x 값들을 계속 바꿔주기

행렬 인수분해 손실 함수 볼록도
- 선형 회귀 손실 함수는 아래로 볼록한 함수
- 행렬 인수분해 손실 함수는 아래로 볼록하지 않음 => 선형 회귀와 달리 변수가 세타, x이기 때문
- 여러 개의 극소점들이 있고 이 중에서 가장 손실이 작은 점이 최소점 => 임의로 값들을 초기화하고 경사 하강법을 해도 손실이 가장 작게 만드는 세타와 x값들을 찾는다고 할 수 없음
-> 손실 함수가 볼록하지 않다는 문제점을 극복하기 위해서는 임의로 초기화를 여러번해서 경사 하강법을 많이 해본 이후, 가장 성능이 좋게 나온 모델을 사용하는 방식을 사용할 수 있음

행렬 인수분해 정규화
- 행렬 인수분해 과적합 -> 손실 함수에 정규화 항을 더해줌으로써 문제 극복 가능
- 정규화를 하기 위해서는 세타와 x들이 커지는 것을 막아야 함. 그러기 위해서 손실 함수에 모든 세타와 x값들의 제곱합 항을 더해주면 됨
