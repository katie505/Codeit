<에다 부스트(Adaboost)>

Boosting
- 일부러 성능이 안 좋은 모델들 사용
- 더 먼저 만든 모델들이 성능이, 뒤에 있는 모델이 사용할 데이터 셋을 바꿈
- 모델들의 예측을 종합할 때, 성능이 좋은 모델의 예측 더 반영
- 핵심 : 성능이 안 좋은 약한 학습자(weak learner)들을 합쳐서 성능 극대화

에다 부스트
- 결정 트리 boosting의 시초 알고리즘
- root 노드 하나와 분류 노드 두 개를 갖는 얕은 결정 트리
1) 성능이 별로 좋지 않은 결정 스텀프들을 많이 만듦
2) 스텀프를 만들 때, 전 스텀프들이 예측에 틀린 데이터들의 중요도를 더 높게 설정
3) 최종 결정을 내릴 때, 성능이 좋은 결정 스텀프들 예측 의견의 비중은 높고, 그렇지 않은 결정 스텀프의 의견의 비중은 낮게 반영

스텀프
- 하나의 질문과 그 질문에 대한 답으로 바로 예측을 하는 결정 트리를 나무의 그루터기
- 이런 식으로 스텀프를 만들게 되면 주로 50%보다 조금 나은 성능을 갖게 됨
- Boosting 기법은 성능이 안 좋은 모델들, weak learner을 사용하는데 이거에 맞게 일부러 일반 결정 트리가 아닌 스텀프만 사용

데이터 셋
- Boosting 기법답게 각 모델이 사용하는 데이터 셋은 임의로 만들지 않음
- 각 스텀프는 전에 있던 스텀프들의 실수를 바로 잡는 방향으로 만들어줌
=> 이걸 미리 정해놓은 만큼 반복해서 엄청 많은 스텀프들을 만들어주는 것

예측
- 에다 부스트는 다수결의 원칙이 아니라 성능 주의적으로 예측함
- 다수결이 2:2여도 더 성능의 합이 높은 결정에 따름

스텀프 성능 계산
- 첫 스텀프 : 전에 만든 스텀프가 없기 때문에 결정 트리를 만들 때랑 똑같이 root 노드를 고름
-> 각 질문들의 지니 불순도 계산 : 지니 불순도가 가장 낮은, 가장 성능을 좋게 하는 질문 기준으로 정하기
- 에다 부스트는 예측을 종합할 때 각 트리의 성능 반영 => 트리를 만들 때마다 미리 성능 계산해야됨
- 특정 스텀프의 성능 = 1/2 * log(1-total_error / total_error)
-> total_error : 잘못 분류한 데이터들의 중요도의 합
=> total_error가 1에 가까워질수록 작아지고, 0에 가까워질수록 커짐
*total_error가 1인 경우 : 스텀프가 모든 데이터를 다 틀리게 예측한 경우 = 성능이 엄청 안 좋음 = 성능을 무한하게 작게 만들기
- 잘 맞출수록, 또는 잘 못 맞출수록 성능을 기하급수적으로 늘리고 줄여주는 것
1) 스텀프는 결정 트리를 만들 때랑 똑같이 지니 불순도 이용해서 만듦
2) 모든 데이터에는 중요도가 있음
3) total_error는 스텀프의 성능을 계산하는 데 사용됨

데이터 중요도 바꾸기
- 틀리게 예측한 데이터의 중요도는 높여주고, 맞게 예측한 데이터의 중요도는 줄이기
= 틀리게 예측한 데이터를 더 잘 분류해야 되기 때문
=> 중요도가 높은 데이터를 더 잘 맞추게 할 것이기 때문
- 틀리게 예측한 데이터의 중요도 = weight(old) * e(Ptree)
- weight(old) : 예전 중요도 / e : 2.71... / Ptree : 스텀프의 성능
- 맞게 예측한 데이터의 중요도 = weight(old) * e(-Ptree)
- ePtree를 그래프로 표현하면 스텀프의 성능이 커질수록 커지고 작아질수록 작아짐
- 전체 중요도의 합은 1이여야함

스텀프 추가하기
- bootstrapping이랑 조금 비슷한데, 임의로 만드는 게 아니라 중요도 이용해서 만들기 때문에 조금 다름
- 새로 만들 데이터 셋은 기존에 데이터 셋의 크기랑 똑같은데, 각 데이터의 중요도를 데이터 셋에 들어갈 확률로 써서 만들기
-> 이걸 하기 위해서 첫 데이터부터 중요도 이용해서 범위 정해줌
EX) 이 데이터는 0부터 중요도 0.1만큼 그 다음은 0.1부터 중요도를 더한 0.2, 그 다음은 0.2부터 0.3 이런 식으로 1까지의 범위를 주
- 그리고 0에서 1사이의 아무 숫자 고르기
-> 이 때 나온 숫자를 갖는 범위의 데이터를 새로운 데이터에 추가
- 중요도가 높은 데이터는 범위도 크기 때문에 선택할 확률이 높음 => 이걸 원래 데이터 셋의 크기만큼 반복. 이렇게 새 데이터 셋 만듦
- 새로운 데이터 셋은 전 스텀프들이 틀린, 중요도가 높은 데이터들이 확률적으로 더 많이 들어있기 때문에 더 잘 맞출 수 있음
- 이제 이 데이터 셋을 써서 첫 스텀프를 만들 때랑 똑같이 새 데이터 셋으로 스텀프 만듦
-> 지니 불순도를 비교해서 가장 성능을 좋게 하는 기준 정하기. 마찬가지로 이 스텀프의 성능 계산. 그 후에는 또 원래 데이터 셋에서 분류에 틀린 데이터들의 중요도를 높여주고, 맞은 데이터의 중요도를 낮춰줌

1) 전 모델이 틀리게 에측한 데이터의 중요도는 올려주고, 맞게 예측한 데이터의 중요도는 낮춤
2) 그 다음에는 데이터의 중요도를 써서, 새로운 데이터 셋을 만들어냄
3) 새로운 데이터 셋으로 스텀프 만들고 이 스텀프의 성능 계산
4) 이렇게 해서 하나의 스텀프 추가. 그 다음부터는 계속 위의 단계들 반복
-> 이런 식으로 스텀프를 만들면 첫 번째 스텀프가 틀리게 분류한 데이터들이 두 번째 스텀프가 좀 더 잘 맞출 수 있고, 세 번째 스텀프는 이 앞에 있는 스텀프들이 틀린 데이터를 좀 더 맞출 수 있고 이런 식으로 스텀프들을 만들 수 있게 됨