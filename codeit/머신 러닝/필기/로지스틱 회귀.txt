<로지스틱 회귀>

분류 문제
- 지도학습 -> 분류

선형 회귀를 이용한 분류
- 선형 회귀는 예외적인 데이터에 너무 민감하게 반응하기 때문에, 보통 데이터를 분류하고 싶을 때는 잘 사용하지 않음

로지스틱 회귀
- 분류를 할 때는 보통 선형 회귀 대신 '로지스틱 회귀' 사용
- 로지스틱 회귀는 데이터에 가장 잘 맞는 일차 함수가 아니라, 데이터에 가장 잘 맞는 시그모이드 함수를 찾는 것
* 시그모이드 함수 : 무조건 0과 1사이의 결과를 냄
=> 선형 회귀에서 쓰는 가설 함수는 일차 함수로 결과가 얼마든지 작아질 수 있고 또 얼마든지 커질 수도 있어서 분류를 하기에 부적절
=> 시그모이드 함수는 많이 동떨어진 데이터 하나가 있어도 크게 영향을 받지 않음

로지스틱 회귀 가설 함수
- 아웃풋이 항상 0과 1사이. 시그모이드 함수

로지스틱 회귀에서 하려는 것
1) 단순화한 시각화
- 선형 회귀와 똑같이 세타 값들을 바꿔가면서 갖고 있는 학습 데이터에 가장 잘 맞는 시그모이드 모양의 곡선을 찾아내는 게 로지스틱 회귀의 목적

2) 변수가 여러 개일 때

결정 경계
- 분류를 할 때, 분류를 구별하는 경계선
- 분류를 하는 모든 문제들에 적용할 수 있는 개념
- 변수가 많아질수록 시각적으로 표현하기 힘들어짐
- 시각적으로 데이터를 나누는 경계선 또는 영역을 그리고 싶을 때 사용

로그 손실
- 로지스틱 회귀의 손실 함수는 평균 제곱 오차 대신 로그 선실 사용함
= cross entropy
- 로그 손실 함수 : 예측값이 실제 결과랑 얼마나 괴리가 있는지 알려주는 역할을 함
=> 아웃풋이 1인 경우에는 예측값이 1에 가까울수록 손실이 0, 멀어질수록 손실이 가파르게 커짐
=> 아웃풋이 0인 경우에는 예측값이 0에 가까울수록 손실이 0, 멀어질수록 손실이 가파르게 커짐

정규 방정식
- 로지스틱 회귀에서는 손실 함수J(로그 손실)가 아래로 볼록(convex)
- 경사 하강법을 사용하면 항상 최적의 세타값을 찾을 수 있음
=> 하지만 J에 대한 편미분 원소들이 선형식이 아님. 기본적으로 세타가 e의 지수에 포함되어있음. 
=> 지수로 포함된 식은 일차식으로만 표현하기 힘듦. 그렇기 때문에 단순 행렬 연산만으로는 최소 지점을 찾아낼 수 없음