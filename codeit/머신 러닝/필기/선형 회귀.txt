<선형 회귀(Linear Regression)>
- 데이터를 가장 잘 대변해 주는 선(= 최적선, line of best fit) 찾아내는 것 
- 목표 변수 = target variable = output variable : 우리가 맞추려고 하는 값
- 입력 변수 = input variable = feature : 목표 변수를 맞추기 위해 사용하는 값

가설 함수
- 가설 함수 : 최적선을 찾아내기 위해 시도하는 함수
- 계수들을 세타로 놓음
- 세타를 조율해서 최적선 찾음

평균 제곱 오차(MSE)
- 가설 함수가 얼마나 좋은지 평가하는 방법
- 데이터들과 가설 함수가 평균적으로 얼마나 떨어져 있는지 나타내기 위한 하나의 방식
- (데이터의 실제 값 - 가설 함수가 예측한 값)^2 = 오차
- MSE = 오차들의 제곱합 / 데이터 개수
- MSE가 크다는 건 가설 함수가 데이터들을 잘 표현하지 못했다는 뜻
** 제곱하는 이유
1) 오차를 계산할 때 양수일 때도 있고 음수일 때도 있는데 똑같이 취급해야하기 때문에
2) 오차가 커질수록 더 부각시키기 위해

손실 함수(loss function)
- 가설 함수를 평가하기 위한 함수
- 아웃풋이 작을수록 가설 함수의 손실이 적다 / 아웃풋이 클수록 손실이 크다
- 손실 함수는 보통 'J'라는 문자를 씀
**선형 회귀의 경우 'MSE'가 손실 함수의 아웃풋
- 손실 함수의 인풋은 세타 = 가설 함수에서 바꿀 수 있는 것은 세타뿐 = 손실 함수의 아웃풋은 세타 값들을 어떻게 설정하느냐에 달려있음

경사 하강법(Gradient Descent)
- 손실 함수의 아웃풋이 최소가 되도록 하는 방법
- 선형 회귀에서 많이 사용
- 기울기에 비례하는 만큼 세타 움직임
- 경사의 방향과 크기를 이용해서 조금씩 극소점을 향해 내려가는 방식(기울기 개선)
- 손실 함수을 각 인풋 변수에 대해서 편미분 -> 편미분한 결과 식에 현재 위치의 좌표를 대입하고 벡터로 만들기 = 현 위치에서의 기울기
=>기울기를 통해서 알 수 있는 것
1) J가 얼마나 기울었는지
2) 현 지점에서 가장 가파르게 올라가는 방향(극대점 방향)
** 극소점으로 향해야하므로 가파르게 내려가는 방향을 알아야하기 때문에 구한 기울기 벡터에 마이너스 붙이면 됨
- 기울기를 개선하는 방법(다음 위치로 이동) : 다음 위치 = 현재 위치에서 각각의 값들에 학습률*기울기를 빼면 됨 = 현 위치 - 학숩률*편미분한 기울기(현 위치 대입)
- 학습률이 크면 많이 움직이는 것 / 작으면 조금씩 움직이는 것

학습률
1) 너무 큰 경우 : 경사 하강을 한 번을 할 때마다 세타 값이 많이 바뀜.
- 경사 하강법을 진행할수록 손실 함수의 최소점에서 멀어질 수도 있음
2) 너무 작은 경우 : 최소 지점을 찾는 데 오래 걸림.
3) 적절한 경우 : 빠르고 정확하게 최소점까지 도달
* 1.0 ~ 0.0 사이의 숫자로 정하고(1, 0.1, 0.01, 0.001 또는 0.5, 0.05, 0.005 이런 식으로) 여러 개를 실험해보면서 경사 하강을 제일 적게 하면서 손실이 잘 줄어드는 학습률 선택

모델 평가하기
- 평균제곱근오차(RMSE)
- 학습과 평가를 위한 데이터를 나눔