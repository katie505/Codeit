<정규화>
편향(bias)과 분산(variance)
1) bias
- bias가 높다 = 모델이 너무 간단해서 데이터의 관계를 잘 학습하지 못하는 경우
- bias가 낮다 = 모델의 복잡도를 늘려서 training 데이터의 관계를 잘 학습할 수 있도록 함(복잡한 곡선이 training 데이터에 거의 완벽히 맞춰짐)
=> 편향이 낮은 모델이 항상 편향이 높은 모델보다 좋다고 할 순 없음

2) variance
- variance가 높다 = 다양한 데이터 셋(train, test) 간에 성능 차이가 많이 남
- variance가 낮다 = 성능이 비슷함
=> 복잡한 곡선 모델은 데이터 셋마다 성능 차이가 많이 나니까 분산이 높음

- 편향이 높은 머신 러닝 모델은 너무 간단해서 주어진 데이터의 관계를 잘 학습하지 못하고 / 편향이 낮은 모델은 주어진 데이터의 관계를 아주 잘 학습함
- 분산은 다양한 테스트가 주어졌을 때 모델의 성능이 얼마나 일관적으로 나오는지를 뜻함
-> 직선 모델은 어떤 데이터 셋에 적용해도 성능이 비슷하게 나옴
-> 복잡한 곡선 모델은 데이터 셋에 따라 성능의 편차가 굉장히 크기 때문에 직선 모델은 분산이 작고, 곡선 모델은 분산이 큼

3) 편향-분산 트레이드오프(Bias-Variance Tradeoff)
- 편향과 분산은 하나가 줄어들수록 다른 하나는 늘어나는 관계
- 머신 러닝 프로그램들의 성능과 밀접한 관계가 있기 때문에 편향과 분산, 다르게는 과소적합과 과적합의 적당한 밸런스를 찾아내는 것이 중요

정규화
- 학습 과정에서 모델이 과적합 되는 것을 예방해줌
- 세타 값들이 너무 커지는 것을 방지해줌 => training 데이터에 대한 오차는 조금 커질 수 있어도, 위아래로 변동이 엄청 심하던 가설 함수를 좀 더 완만하게 만들 수 있음
- 손실함수에 '정규화 항'이라는 것을 더해서 세타값들이 커지는 것을 방지함
- 과적합된 함수는 보통 위아래로 엄청 왔다 갔다하는 특징이 있음
=> 많은 굴곡을 이용해서 함수가 training 데이터를 최대한 많이 통과하도록 하는 것
=> 함수가 이렇게 급격하게 변화한다는 건 함수의 계수, 즉 가설 함수의 세타 값들이 굉장이 크다
- 모델의 파라미터(즉 학습을 통해 찾고자 하는 값들 - 회귀의 경우 '세타')에 대한 손실 함수를 최소화하는 모든 알고리즘에 적용할 수 있음
=> 다중 회귀, (다중) 다항 회귀, 로지스틱 회귀 모델 모두에 정규화 적용 가능

- L1 정규화 : 손실함수에 람다*(세타값들의 절대값 또는 크기의 합) 더함
*람다 : 세타값들이 커지는 것에 대해서 얼마나 많은 패널티를 줄 건지 정함
=> Lasso 회귀 모델
- L2 정규화 : 손실함수에 람다*(세타값들의 제곱 값의 함) 더함
=> Ridge 회귀 모델 또는 Ridge 모델

*다중 회귀 또는 다항 회귀 모델을 만들 때는 LinearRegrssion 대신 L1, L2 정규화 모델 사용
1) L1, L2 정규화 일반화
- LogisticRegression모델은 자동으로 L2 정규화 적용
- 어떤 정규화 기법을 사용할 지는 모델의 penalty라는 옵셔널 파라미터로 정해 줄 수 있음
EX) 
LogisticRegression(penalty='none')  # 정규화 사용 안함
LogisticRegression(penalty='l1')  # L1 정규화 사용
LogisticRegression(penalty='l2')  # L2 정규화 사용
LogisticRegression()  # 위와 똑같음: L2 정규화 사용

2) L1, L2 정규화 차이점
- L1 정규화는 여러 세타 값들을 0으로 만들어 줌. 모델에 중요하지 않다고 생각되는 속성들을 아예 없애주는 것
=> 어떤 모델에 쓰이는 속성 또는 변수의 개수를 줄이고 싶을 때 사용
-> 속성이 많으면 과적합 뿐만 아니라 모델을 학습시킬 때 많은 자원(RAM, 시간 등)을 소모할 수 있음

- L2 정규화는 세타 값들을 0으로 만들기보다는 조금씩 줄여 줌. 모델에 사용되는 속성들을 L1처럼 없애지는 않음
=> 딱히 속성의 개수를 줄일 필요가 없다고 생각되면 사용
