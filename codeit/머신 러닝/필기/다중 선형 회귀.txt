<다중 선형 회귀>
- 선형 회귀를 하나의 입력 변수가 아니라 여러 개의 입력 변수를 사용해서 목표 변수를 예측하는 알고리즘

다중 선형 회귀 문제 표현
- 입력 변수 = 속성 여러 개

가설 함수
- 벡터, 행렬을 사용하여 표현

경사하강법 구현
error = X@theta - y 
theta = theta - alpha*m(X.T@error)

평균 제곱 오차 = 모든 데이터의 제곱 오차의 합을 행렬식으로 표현
= (X@theta - y).T @ (X@theta-y)
- 손실 함수 J(theta) = 1/2m*(X@theta - y).T @ (X@theta-y)

정규 방정식
- 손실 함수의 미분 함수의 기울기가 0이 되는 theta 값을 구하는 식
- ∇J(theta) = 0의 해를 구하는 것
- theta = (X.T@X)^(-1) @ X.T @ y

선형 회귀의 손실 함수를 최소화하는 세타를 찾는 방법
1) 경사 하강법
- 적합한 학습율을 찾거나 정해야 함
- 반복문을 사용해야 함
- 입력 변수의 개수 n이 커도 효율적으로 연산할 수 있음

2) 정규 방정식
- 학습율을 정할 필요가 없음
- 한 단계로 계산을 끝낼 수 있음
- 입력 변수의 개수 n이 커지면 커질수록 월등히 비효율적(행렬 연산을 하는 비용이 경사 하강법 하는 것보다 크다)
- 역행렬이 존재하지 않을 수도 있다(pseudo inverse를 이용해서 다르게 계산하는 방법이 있기 때문에 큰 문제는 안 됨)
*입력 변수의 수가 엄청 많을 땐 경사 하강법, 비교적 입력 변수의 수가 적을 때는 정규 방정식

Convex 함수
- 아래로 볼록한 함수
- 어떤 지점에서 경사 하강을 시작해도 항상 손실 함수의 최소 지점을 찾을 수 있고, 정규 방정식을 이용해서 최소점을 구할 수 있음
- convex 함수에서는 항상 경사 하강법이나 정규 방정식을 이용해서 최소점을 구할 수 있는 반면, non-convex 함수에서는 구한 극소점이 최소점이라고 확신할 수 없음
*선형 회귀 손실 함수로 사용하는 MSE는 항상 convex 함수